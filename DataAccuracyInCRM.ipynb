{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeOd5BDzqWrc7e8CsbEIt6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niveditanayak2003/python-jupyter-template/blob/main/DataAccuracyInCRM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nNt3kpPCp8nq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "8fc957eb-4cbc-4c71-c958-8829b4f2eb6d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9154481be9bc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.impute import KNNImputer, SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import silhouette_score, confusion_matrix\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/My Drive/Dataset/uncleaned_crm_data.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Ensure dataset is not empty\n",
        "if data.empty:\n",
        "    raise ValueError(\"Error: Dataset is empty or not loaded properly!\")\n",
        "\n",
        "# Ensure required columns exist\n",
        "required_columns = {'email', 'name', 'age', 'valid'}\n",
        "missing_cols = required_columns - set(data.columns)\n",
        "if missing_cols:\n",
        "    print(f\"Warning: The following columns are missing in the dataset: {missing_cols}\")\n",
        "\n",
        "# Correct misspelled 'gmail' in email addresses\n",
        "def correct_gmail(email):\n",
        "    if isinstance(email, str):\n",
        "        return re.sub(r'\\b(gmial|gmaill|gmai|gmal|gmaul|gmeil)\\.com\\b', 'gmail.com', email, flags=re.IGNORECASE)\n",
        "    return email\n",
        "\n",
        "if 'email' in data.columns:\n",
        "    data['email'] = data['email'].str.lower().apply(correct_gmail)\n",
        "\n",
        "# Fill missing names using email prefix\n",
        "def extract_name_from_email(email):\n",
        "    if isinstance(email, str) and '@' in email:\n",
        "        return email.split('@')[0]\n",
        "    return 'Unknown'\n",
        "\n",
        "if 'name' in data.columns and 'email' in data.columns:\n",
        "    data['name'].fillna(data['email'].apply(lambda x: extract_name_from_email(x)), inplace=True)\n",
        "\n",
        "# Remove duplicates using TF-IDF & DBSCAN\n",
        "if 'name' in data.columns and 'email' in data.columns:\n",
        "    data['combined'] = data['name'].fillna('') + ' ' + data['email'].fillna('')\n",
        "\n",
        "    # Apply TF-IDF\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(data['combined'])\n",
        "\n",
        "    # DBSCAN Clustering with dynamic epsilon selection\n",
        "    eps_values = np.linspace(0.3, 1.0, 5)\n",
        "    best_eps, best_score = None, -1\n",
        "\n",
        "    for eps in eps_values:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=2, metric='cosine')\n",
        "        clusters = dbscan.fit_predict(X)\n",
        "\n",
        "        # Only compute silhouette score if more than one cluster\n",
        "        if len(set(clusters)) > 1:\n",
        "            score = silhouette_score(X, clusters)\n",
        "            if score > best_score:\n",
        "                best_score, best_eps = score, eps\n",
        "\n",
        "    if best_eps:\n",
        "        data['cluster'] = DBSCAN(eps=best_eps, min_samples=2, metric='cosine').fit_predict(X)\n",
        "    else:\n",
        "        print(\"Warning: DBSCAN did not find multiple clusters.\")\n",
        "\n",
        "# Missing Value Imputation with Linear Regression & KNN\n",
        "if 'age' in data.columns:\n",
        "    train_data = data[data['age'].notnull()]\n",
        "    predict_data = data[data['age'].isnull()]\n",
        "\n",
        "    if not train_data.empty and not predict_data.empty:\n",
        "        X_train = pd.get_dummies(train_data.drop(columns=['age', 'combined', 'cluster'], errors='ignore'), drop_first=True)\n",
        "        y_train = train_data['age']\n",
        "\n",
        "        # Ensure no missing values in training features\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
        "\n",
        "        model = LinearRegression()\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Prepare data for prediction\n",
        "        X_predict = pd.get_dummies(predict_data.drop(columns=['age', 'combined', 'cluster'], errors='ignore'), drop_first=True)\n",
        "        X_predict = X_predict.reindex(columns=X_train.columns, fill_value=0)\n",
        "        X_predict = pd.DataFrame(imputer.transform(X_predict), columns=X_predict.columns)\n",
        "\n",
        "        # Predict missing ages\n",
        "        data.loc[data['age'].isnull(), 'age'] = model.predict(X_predict)\n",
        "\n",
        "# Apply KNN Imputation for remaining missing values\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "data[numeric_columns] = knn_imputer.fit_transform(data[numeric_columns])\n",
        "\n",
        "# Anomaly Detection using Decision Tree & Logistic Regression\n",
        "if 'valid' in data.columns:\n",
        "    X = pd.get_dummies(data.drop(columns=['valid', 'combined', 'cluster'], errors='ignore'), drop_first=True)\n",
        "    y = data['valid']\n",
        "\n",
        "    # Handle missing values before training\n",
        "    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    dt_clf = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "    dt_clf.fit(X_train, y_train)\n",
        "    dt_predictions = dt_clf.predict(X_test)\n",
        "\n",
        "    log_reg = LogisticRegression()\n",
        "    log_reg.fit(X_train, y_train)\n",
        "    log_predictions = log_reg.predict(X_test)\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, dt_predictions)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Save the cleaned dataset\n",
        "cleaned_file_path = '/content/drive/My Drive/Dataset/cleaned_data.xlsx'\n",
        "data.to_excel(cleaned_file_path, index=False)\n",
        "print(f\"Cleaned data saved to: {cleaned_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "\n",
        "# 🔹 Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 🔹 Define file paths\n",
        "file_path_old = '/content/drive/My Drive/Dataset/uncleaned_crm_data.xlsx'  # Original dataset\n",
        "file_path_cleaned = '/content/drive/My Drive/Dataset/cleaned_data.xlsx'  # Cleaned dataset\n",
        "\n",
        "# 🔹 Check if cleaned data exists before loading\n",
        "if os.path.exists(file_path_old):\n",
        "    data_old = pd.read_excel(file_path_old)\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Error: The original dataset file was not found at {file_path_old}\")\n",
        "\n",
        "if os.path.exists(file_path_cleaned):\n",
        "    data_cleaned = pd.read_excel(file_path_cleaned)\n",
        "else:\n",
        "    print(f\"Warning: Cleaned dataset file not found at {file_path_cleaned}. Proceeding with original data only.\")\n",
        "    data_cleaned = None  # Handle gracefully\n",
        "\n",
        "# 🔹 Display first few rows\n",
        "print(\"Original Dataset Loaded:\\n\", data_old.head())\n",
        "\n",
        "if data_cleaned is not None:\n",
        "    print(\"\\nCleaned Dataset Loaded:\\n\", data_cleaned.head())\n",
        "\n",
        "# 🔹 Ensure column names are lowercase for consistency\n",
        "data_old.columns = data_old.columns.str.lower()\n",
        "if data_cleaned is not None:\n",
        "    data_cleaned.columns = data_cleaned.columns.str.lower()\n",
        "\n",
        "# ✅ Visualization 1: Missing Values Count\n",
        "plt.figure(figsize=(8, 6))\n",
        "data_old.isnull().sum().plot(kind='bar', color='red', edgecolor='black', alpha=0.7, label='Original Data')\n",
        "\n",
        "if data_cleaned is not None:\n",
        "    data_cleaned.isnull().sum().plot(kind='bar', color='green', edgecolor='black', alpha=0.7, label='Cleaned Data')\n",
        "\n",
        "plt.title(\"Missing Values per Column\", fontsize=14)\n",
        "plt.xlabel(\"Columns\", fontsize=12)\n",
        "plt.ylabel(\"Count of Missing Values\", fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# ✅ Visualization 2: Duplicate Records Count\n",
        "duplicates_old = data_old.duplicated().sum()\n",
        "duplicates_cleaned = data_cleaned.duplicated().sum() if data_cleaned is not None else 0\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar([\"Original Data\", \"Cleaned Data\"], [duplicates_old, duplicates_cleaned], color=[\"red\", \"green\"], edgecolor=\"black\")\n",
        "plt.title(\"Duplicate Records Count\", fontsize=14)\n",
        "plt.ylabel(\"Count\", fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# ✅ Visualization 3: Age Distribution Before & After Cleaning\n",
        "if 'age' in data_old.columns and (data_cleaned is None or 'age' in data_cleaned.columns):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(data_old['age'].dropna(), kde=True, color='red', bins=20, label='Original Data')\n",
        "\n",
        "    if data_cleaned is not None:\n",
        "        sns.histplot(data_cleaned['age'].dropna(), kde=True, color='green', bins=20, label='Cleaned Data')\n",
        "\n",
        "    plt.title(\"Age Distribution Before & After Cleaning\")\n",
        "    plt.xlabel(\"Age\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', linestyle='--')\n",
        "    plt.show()\n",
        "\n",
        "# ✅ Visualization 4: Email Domain Distribution Before & After Cleaning\n",
        "if 'email' in data_old.columns and (data_cleaned is None or 'email' in data_cleaned.columns):\n",
        "    data_old['email_domain'] = data_old['email'].str.split('@').str[-1]\n",
        "    if data_cleaned is not None:\n",
        "        data_cleaned['email_domain'] = data_cleaned['email'].str.split('@').str[-1]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    data_old['email_domain'].value_counts().nlargest(10).plot(kind='bar', color='red', edgecolor='black')\n",
        "    plt.title(\"Top Email Domains - Original Data\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(axis='y', linestyle='--')\n",
        "\n",
        "    if data_cleaned is not None:\n",
        "        plt.subplot(1, 2, 2)\n",
        "        data_cleaned['email_domain'].value_counts().nlargest(10).plot(kind='bar', color='green', edgecolor='black')\n",
        "        plt.title(\"Top Email Domains - Cleaned Data\")\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.grid(axis='y', linestyle='--')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ✅ Visualization 5: City Distribution Comparison\n",
        "if 'city' in data_old.columns and (data_cleaned is None or 'city' in data_cleaned.columns):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    data_old['city'].value_counts().nlargest(10).plot(kind='bar', color='red', edgecolor='black')\n",
        "    plt.title(\"Top 10 Cities - Original Data\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.grid(axis='y', linestyle='--')\n",
        "\n",
        "    if data_cleaned is not None:\n",
        "        plt.subplot(1, 2, 2)\n",
        "        data_cleaned['city'].value_counts().nlargest(10).plot(kind='bar', color='green', edgecolor='black')\n",
        "        plt.title(\"Top 10 Cities - Cleaned Data\")\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.ylabel(\"Count\")\n",
        "        plt.grid(axis='y', linestyle='--')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ✅ Save a summary of missing and duplicate records (if cleaned dataset exists)\n",
        "if data_cleaned is not None:\n",
        "    summary = {\n",
        "        \"Total Records\": [len(data_cleaned)],\n",
        "        \"Duplicate Records\": [duplicates_cleaned],\n",
        "        \"Missing Values (Total)\": [data_cleaned.isnull().sum().sum()]\n",
        "    }\n",
        "    summary_df = pd.DataFrame(summary)\n",
        "    summary_file_path = '/content/drive/My Drive/Dataset/summary_report.xlsx'\n",
        "    summary_df.to_excel(summary_file_path, index=False)\n",
        "    print(f\"\\nSummary report saved to: {summary_file_path}\")\n",
        "\n",
        "print(\"\\n✅ CRM Data Visualization Completed Successfully!\")\n"
      ],
      "metadata": {
        "id": "Z1gB7EKvylfk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}